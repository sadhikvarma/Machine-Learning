{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-grams: [('This',), ('is',), ('a',), ('sample',), ('sentence',), ('for',), ('generating',), ('n-grams',), ('.',)]\n",
      "Bi-grams: [('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence'), ('sentence', 'for'), ('for', 'generating'), ('generating', 'n-grams'), ('n-grams', '.')]\n",
      "Tri-grams: [('This', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'sentence'), ('sample', 'sentence', 'for'), ('sentence', 'for', 'generating'), ('for', 'generating', 'n-grams'), ('generating', 'n-grams', '.')]\n",
      "4-grams: [('This', 'is', 'a', 'sample'), ('is', 'a', 'sample', 'sentence'), ('a', 'sample', 'sentence', 'for'), ('sample', 'sentence', 'for', 'generating'), ('sentence', 'for', 'generating', 'n-grams'), ('for', 'generating', 'n-grams', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "def main():\n",
    "    # Example text\n",
    "    text = \"This is a sample sentence for generating n-grams.\"\n",
    "\n",
    "    # Uni-grams\n",
    "    uni_grams = generate_ngrams(text, 1)\n",
    "    print(\"Uni-grams:\", uni_grams)\n",
    "\n",
    "    # Bi-grams\n",
    "    bi_grams = generate_ngrams(text, 2)\n",
    "    print(\"Bi-grams:\", bi_grams)\n",
    "\n",
    "    # Tri-grams\n",
    "    tri_grams = generate_ngrams(text, 3)\n",
    "    print(\"Tri-grams:\", tri_grams)\n",
    "\n",
    "    # N-grams (you can specify any value for N)\n",
    "    n_value = 4\n",
    "    n_grams = generate_ngrams(text, n_value)\n",
    "    print(f\"{n_value}-grams:\", n_grams)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams_user_defined(text, n):\n",
    "    words = text.split()\n",
    "    n_grams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    return n_grams\n",
    "\n",
    "def main():\n",
    "    # Example text\n",
    "    text = \"This is a sample sentence for generating n-grams.\"\n",
    "\n",
    "    # Uni-grams\n",
    "    uni_grams = generate_ngrams_user_defined(text, 1)\n",
    "    print(\"Uni-grams:\", uni_grams)\n",
    "\n",
    "    # Bi-grams\n",
    "    bi_grams = generate_ngrams_user_defined(text, 2)\n",
    "    print(\"Bi-grams:\", bi_grams)\n",
    "\n",
    "    # Tri-grams\n",
    "    tri_grams = generate_ngrams_user_defined(text, 3)\n",
    "    print(\"Tri-grams:\", tri_grams)\n",
    "\n",
    "    # N-grams (you can specify any value for N)\n",
    "    n_value = 4\n",
    "    n_grams = generate_ngrams_user_defined(text, n_value)\n",
    "    print(f\"{n_value}-grams:\", n_grams)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Words: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"This is a sample sentence for tokenization.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Tokenized Words:\", tokenized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-grams: [('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence'), ('sentence', 'for'), ('for', 'generating'), ('generating', 'n-grams.')]\n",
      "Tri-grams: [('This', 'is', 'a'), ('is', 'a', 'sample'), ('a', 'sample', 'sentence'), ('sample', 'sentence', 'for'), ('sentence', 'for', 'generating'), ('for', 'generating', 'n-grams.')]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(sequence, n):\n",
    "    return [tuple(sequence[i:i + n]) for i in range(len(sequence) - n + 1)]\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample sentence for generating n-grams.\"\n",
    "words = text.split()\n",
    "\n",
    "# Generating bi-grams\n",
    "bi_grams = generate_ngrams(words, 2)\n",
    "print(\"Bi-grams:\", bi_grams)\n",
    "\n",
    "# Generating tri-grams\n",
    "tri_grams = generate_ngrams(words, 3)\n",
    "print(\"Tri-grams:\", tri_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest probability is: 1.00 for the pair: ('this', 'one')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 4 samples and 5 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "bigrams_list = [('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'sentence'),('a', 'sample') ]\n",
    "\n",
    "freq_dist = FreqDist(bigrams_list)\n",
    "print(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word pair with the highest probability is: ('It', 'was') with a probability of 1.00\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define a function to calculate the highest probability of a word (w2) occurring after another word (w1).\n",
    "def highest_probability(text):\n",
    "    # Tokenize the input text into words.\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Create a dictionary to store the occurrences of each word and its preceding word.\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Calculate the conditional probability of each word pair.\n",
    "    for word1, word2 in ngrams(words, 2):\n",
    "        counts[word1][word2] += 1\n",
    "        counts[word1]['total'] += 1\n",
    "\n",
    "    # Find the word pair with the highest probability.\n",
    "    max_probability = 0\n",
    "    max_word_pair = None\n",
    "    for word1 in counts:\n",
    "        for word2 in counts[word1]:\n",
    "            if word2 != 'total':\n",
    "                probability = counts[word1][word2] / counts[word1]['total']\n",
    "                if probability > max_probability:\n",
    "                    max_probability = probability\n",
    "                    max_word_pair = (word1, word2)\n",
    "\n",
    "    return max_word_pair, max_probability\n",
    "\n",
    "# Example usage:\n",
    "text = \"It was the best of times, it  the worst of times.\"\n",
    "word_pair, probability = highest_probability(text)\n",
    "print(f\"The word pair with the highest probability is: {word_pair} with a probability of {probability:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConditionalFreqDist with 10 conditions>\n",
      "[('This', 'is'), ('is', 'a'), ('a', 'sample'), ('sample', 'text'), ('text', '.'), ('.', 'This'), ('This', 'text'), ('text', 'is'), ('is', 'for'), ('for', 'testing'), ('testing', 'probability'), ('probability', 'calculation'), ('calculation', '.')]\n",
      "[('text', 'is')]\n",
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "\n",
    "def calculate_highest_probability(text, w1, w2):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    bigrams_list = list(bigrams(words))\n",
    "\n",
    "    cfd = ConditionalFreqDist(bigrams_list)\n",
    "    print(cfd)\n",
    "    \n",
    "    # Get the words that follow w1\n",
    "    next_words = [(w1,w2) for word1, word2 in bigrams_list if word1 == w1 and word2 == w2]\n",
    "    \n",
    "    print(bigrams_list)\n",
    "    # print(bigram_freq_dist)\n",
    "    print(next_words)\n",
    "\n",
    "    print(len(next_words)/len(bigrams_list))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is a sample text. This text is for testing probability calculation.\"\n",
    "w1 = \"text\"\n",
    "w2 = \"is\"\n",
    "\n",
    "highest_prob_word = calculate_highest_probability(text, w1,w2)\n",
    "\n",
    "# if highest_prob_word:\n",
    "#     print(f\"The word with the highest probability after '{w1}' is: {highest_prob_word}\")\n",
    "# else:\n",
    "#     print(f\"There is no word after '{w1}' in the provided text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This should is a sample text. This should  text is for testing probability calculation.\n",
      "The bigram with the highest frequency after 'this' is: None\n",
      "It occurs 0 times in the corpus.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "# Load the Reuters corpus\n",
    "corpus =\"This should is a sample text. This should  text is for testing probability calculation.\"\n",
    "# Get all bigrams in the corpus\n",
    "bigrams = list(ngrams([word.lower() for sentence in corpus for word in sentence], 2))\n",
    "# Calculate the frequency of each bigram\n",
    "freq_dist = FreqDist(bigrams)\n",
    "# Define the starting word (w1)\n",
    "w1 = 'This'\n",
    "# Get all bigrams starting with w1\n",
    "w1_bigrams = [(w1, b[1]) for b in bigrams if b[0] == w1]\n",
    "# Get the bigram with the highest frequency\n",
    "max_freq = 0\n",
    "max_bigram = None\n",
    "for bigram in w1_bigrams:\n",
    "    freq = freq_dist[bigram]\n",
    "    if freq > max_freq:\n",
    "        max_freq = freq\n",
    "        max_bigram= bigram\n",
    "\n",
    "# Print the bigram with the highest frequency\n",
    "print(corpus)\n",
    "print(\"The bigram with the highest frequency after 'this' is:\", max_bigram)\n",
    "print(\"It occurs\", max_freq, \"times in the corpus.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
