{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\tthis: 0.04794701207529681\n",
      "\tis: -0.03719059188570162\n",
      "\tthe: -0.03719059188570162\n",
      "\tfirst: 0.04794701207529681\n",
      "\tdocument: 0.0\n",
      "\t.: 0.0\n",
      "Document 2:\n",
      "\tthis: 0.04109743892168297\n",
      "\tdocument: 0.0\n",
      "\tis: -0.031877650187744244\n",
      "\tthe: -0.031877650187744244\n",
      "\tsecond: 0.09902102579427789\n",
      "\t.: 0.0\n",
      "Document 3:\n",
      "\tand: 0.19804205158855578\n",
      "\tthis: 0.04109743892168297\n",
      "\tis: -0.031877650187744244\n",
      "\tthe: -0.031877650187744244\n",
      "\tthird: 0.09902102579427789\n",
      "\tone: 0.09902102579427789\n",
      "\t.: 0.0\n",
      "Document 4:\n",
      "\tis: -0.03719059188570162\n",
      "\tthis: 0.04794701207529681\n",
      "\tthe: -0.03719059188570162\n",
      "\tfirst: 0.04794701207529681\n",
      "\tdocument: 0.0\n",
      "\t?: 0.11552453009332421\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Sample corpus (replace with your corpus)\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "def calculate_tf(document):\n",
    "    # Tokenize the document\n",
    "    tokens = nltk.word_tokenize(document.lower())\n",
    "    # Calculate term frequency (TF)\n",
    "    tf = Counter(tokens)\n",
    "    # Normalize TF by dividing by the total number of terms\n",
    "    total_terms = len(tokens)\n",
    "    tf_normalized = {term: freq/total_terms for term, freq in tf.items()}\n",
    "    return tf_normalized\n",
    "\n",
    "def calculate_idf(corpus):\n",
    "    # Get unique terms in the corpus\n",
    "    all_tokens = [token for document in corpus for token in nltk.word_tokenize(document.lower())]\n",
    "    unique_tokens = set(all_tokens)\n",
    "    # Calculate inverse document frequency (IDF)\n",
    "    idf = {}\n",
    "    num_documents = len(corpus)\n",
    "    for token in unique_tokens:\n",
    "        num_documents_containing_token = sum([1 for document in corpus if token in document])\n",
    "        idf[token] = math.log(num_documents / (1 + num_documents_containing_token))\n",
    "    return idf\n",
    "\n",
    "def calculate_tfidf(corpus):\n",
    "    tfidf_scores = []\n",
    "    idf = calculate_idf(corpus)\n",
    "    for document in corpus:\n",
    "        tf = calculate_tf(document)\n",
    "        tfidf = {term: tf[term] * idf[term] for term in tf.keys()}\n",
    "        tfidf_scores.append(tfidf)\n",
    "    return tfidf_scores\n",
    "\n",
    "# Calculate TF-IDF scores for the corpus\n",
    "tfidf_scores = calculate_tfidf(corpus)\n",
    "\n",
    "# Print TF-IDF scores for each document\n",
    "for i, tfidf in enumerate(tfidf_scores):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    for term, score in tfidf.items():\n",
    "        print(f\"\\t{term}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
